{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38530/1601242867.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# from inducingPointMethods import SVGD_inducing_points, kMeans_inducing_points, random_inducing_points, GFF_inducing_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minducingPointSampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglobalModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mSABER\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/modelagnosic_superior_training/inducingPointSampler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minducingPointMethods\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVGD_inducing_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkMeans_inducing_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_inducing_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGFF_inducing_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitIPsamplerPars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipSelectionPars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malPars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelPars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/modelagnosic_superior_training/inducingPointMethods.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msvgd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmyKMeans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmyKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mAL_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmyScientificFormat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/modelagnosic_superior_training/myKMeans.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstable_cumsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m from .utils._tags import (\n\u001b[1;32m     19\u001b[0m     \u001b[0m_DEFAULT_TAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/_joblib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# joblib imports may raise DeprecationWarning on certain Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMemorizedResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_store_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrintTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunc_inspect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mformat_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_store_backends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStoreBackendBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileSystemStoreBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/_store_backends.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcurrency_safe_rename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdisk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmkdirp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemstr_to_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrm_subdirs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m CacheItemInfo = collections.namedtuple('CacheItemInfo',\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompressor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlz4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLZ4_NOT_INSTALLED_ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompressor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_COMPRESSORS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_compressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBinaryZlibFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m from .compressor import (ZlibCompressorWrapper, GzipCompressorWrapper,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/compressor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mlz4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mlz4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLZ4FrameFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.append(\"../modelagnosic_superior_training/\")\n",
    "import warnings\n",
    "# filters the nan-median slicing warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "# from inducingPointMethods import SVGD_inducing_points, kMeans_inducing_points, random_inducing_points, GFF_inducing_points\n",
    "from inducingPointSampler import *\n",
    "from globalModel import *\n",
    "from SABER import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from AL_base import *\n",
    "from AL_framework import saberActiveLearner\n",
    "\n",
    "saveDir = 'doppler'\n",
    "dataName = 'doppler'\n",
    "\n",
    "inputDim = 1\n",
    "inputSpaceBounds = np.array([[0.]*inputDim,[1.]*inputDim])\n",
    "\n",
    "xPlot = equidistantInput(10000, inputSpaceBounds)\n",
    "\n",
    "testDensity = inputDensityUnif\n",
    "randTestDistribution = randInputUnif\n",
    "testDensityPlot = testDensity(xPlot, inputSpaceBounds)\n",
    "\n",
    "eps = 0.05\n",
    "a = 1/eps\n",
    "bb = 1 + 1/eps\n",
    "def f(x):\n",
    "    return(a*x + 1)\n",
    "def labelFun(x):\n",
    "    return((x*(1-x))**0.5*np.sin(2*np.pi*bb/f(x)))\n",
    "yPlot = labelFun(xPlot)\n",
    "labelScaling = 7/np.std(yPlot)\n",
    "def labelFun(x):\n",
    "    return(labelScaling*(x*(1-x))**0.5*np.sin(2*np.pi*bb/f(x)))\n",
    "\n",
    "noiseStdDev = 1\n",
    "def localNoiseVar(x):\n",
    "    return(np.ones(len(x))*noiseStdDev**2)\n",
    "# TODO need to decide where a local noise variance function would be placed best\n",
    "localNoiseVariance = None\n",
    "\n",
    "if False:\n",
    "    noiseStdDevHigh = 3\n",
    "    noiseStdDevLow = 1\n",
    "    def localNoiseVar(x):\n",
    "        return(((noiseStdDevHigh-noiseStdDevLow)*(1-2*np.abs(x.ravel() - 0.5*np.ones(len(x)))) + noiseStdDevLow)**2)\n",
    "    localNoiseVariance = localNoiseVar\n",
    "    \n",
    "def noisyLabelFun(x):\n",
    "    return(labelFun(x) + (localNoiseVar(x)**0.5*np.random.normal(size=len(x))).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vPlot = localNoiseVar(xPlot)\n",
    "yPlot = labelFun(xPlot)\n",
    "\n",
    "n = int(2**10)\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "currentX = randInputUnif(n, inputSpaceBounds)\n",
    "currentY = noisyLabelFun(currentX)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "fig.set_figheight(16)\n",
    "fig.set_figwidth(16)\n",
    "ax[0].set_xlabel('x', fontsize=24)\n",
    "ax[0].set_xlim(0,1)\n",
    "ax[0].set_ylim(-14, 14)\n",
    "ax[0].plot(xPlot, yPlot, 'k-',label='f')\n",
    "ax[0].scatter(currentX, currentY, color = \"blue\", alpha=0.1,label='y')\n",
    "############\n",
    "ax[1].set_xlabel('log-scale x', fontsize=24)\n",
    "ax[1].set_xlim(0.001, 1.)\n",
    "ax[1].set_ylim(-14, 14)\n",
    "ax[1].semilogx(xPlot, yPlot, 'k-',label='f')\n",
    "ax[1].scatter(currentX, currentY, color = \"blue\", alpha=0.1,label='y')\n",
    "############\n",
    "\n",
    "##########\n",
    "ax[1].legend(loc='upper left', fontsize = 20)\n",
    "plt.draw()\n",
    "plt.pause(2)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ reasonable parameter groups #################\n",
    "\n",
    "# experimental parameters\n",
    "def initExpPars():\n",
    "    expPars = {}\n",
    "    expPars['validationSize'] = None\n",
    "    expPars['valLoss'] = 'mll'\n",
    "    expPars['trainingSizes'] = []\n",
    "    expPars['iters'] = 0\n",
    "    expPars['repetitions'] = 1\n",
    "    expPars['splitRatio'] = 1\n",
    "    expPars['labelModel'] = True\n",
    "    expPars['derivativeModel'] = False\n",
    "    return expPars\n",
    "expPars = initExpPars()\n",
    "expPars['validationSize'] = int(2**10)\n",
    "expPars['trainingSizes'] = []\n",
    "for i in range(9,16):\n",
    "    expPars['trainingSizes'].append(int(2**i))\n",
    "expPars['iters'] = len(expPars['trainingSizes'])\n",
    "\n",
    "# AL-related parameters\n",
    "def initALPars():\n",
    "    alPars = {}\n",
    "    alPars['initialSamplingStrategy'] = 'random'\n",
    "    alPars['Q'] = np.Inf\n",
    "    alPars['inputDim'] = None\n",
    "    alPars['inputSpaceBounds'] = None\n",
    "    alPars['intrinsicDim'] = None\n",
    "    alPars['updateTrainingDensity'] = True\n",
    "    alPars['samplingAlgorithm'] = 'Random' # how to sample from proposed superior training density after update\n",
    "    alPars['noiseFree'] = False\n",
    "    alPars['homoscedastic'] = True\n",
    "    alPars['correctLFCforNoise'] = True\n",
    "    return alPars\n",
    "    \n",
    "alPars = initALPars()\n",
    "alPars['initialSamplingStrategy'] = 'equi'\n",
    "alPars['inputDim'] = inputDim\n",
    "alPars['inputSpaceBounds'] = inputSpaceBounds\n",
    "alPars['correctLFCforNoise'] = False\n",
    "alPars['homoscedastic'] = localNoiseVariance is None\n",
    "\n",
    "def initIPselectionPars():\n",
    "    ipSelectionPars = {}\n",
    "    # inducing point initialization strategy\n",
    "    ipSelectionPars['ipComplexityExponent'] = 0.\n",
    "    ipSelectionPars['IPstrategyExpert'] = 'Random' # 'SVGD', 'Random', 'KMeans', 'GFF'\n",
    "    ipSelectionPars['IPstrategyGate'] = 'Random' # NOTE: Cannot be 'GFF' due to missing labels\n",
    "    # SVGD related parameters\n",
    "    ipSelectionPars['svgdIters'] = 200\n",
    "    ipSelectionPars['svgdRepulsionPropToDensity'] = True\n",
    "    ipSelectionPars['svgdVersion'] = 'v2'\n",
    "    ipSelectionPars['svgdIPrefSize'] = None\n",
    "    ipSelectionPars['svgdSigma'] = 1e0\n",
    "    ipSelectionPars['svgdInitStepSize'] = 1e-1\n",
    "    ipSelectionPars['svgdFinalStepSize'] = 1e-3\n",
    "\n",
    "    ipSelectionPars['KMeansInit'] = 'k-means++' # 'random', 'k-means++'\n",
    "    ipSelectionPars['KMeansMaxIter'] = 300\n",
    "    ipSelectionPars['KMeansDistributional'] = True\n",
    "    \n",
    "    ipSelectionPars['GFFthreshold'] = 1e-2\n",
    "    return ipSelectionPars\n",
    "\n",
    "ipSelectionPars = initIPselectionPars()\n",
    "ipSelectionPars['ipComplexityExponent'] = 0.5\n",
    "\n",
    "ipSelectionPars['IPstrategyExpert'] = 'SVGD'\n",
    "ipSelectionPars['IPstrategyGate'] = 'SVGD'\n",
    "ipSelectionPars['svgdInitStepSize'] = 2e-3\n",
    "ipSelectionPars['svgdFinalStepSize'] = 2e-5\n",
    "ipSelectionPars['svgdSigma'] = 2e-3\n",
    "ipSelectionPars['svgdIPrefSize'] = expPars['trainingSizes'][0]\n",
    "\n",
    "def initModelPars():\n",
    "    modelPars = {}\n",
    "    \n",
    "    # general model design and training\n",
    "    modelPars['maxSizeAnalytic'] = None\n",
    "    modelPars['num_workers'] = None\n",
    "    modelPars['max_processes'] = None\n",
    "    modelPars['minEPOCH'] = 0\n",
    "    modelPars['maxEPOCH'] = np.Inf\n",
    "    \n",
    "    modelPars['plotFrequencyFactor'] = 5\n",
    "    modelPars['patienceFactor'] = 1\n",
    "    modelPars['validationFrequency'] = 1\n",
    "    modelPars['subValidationFrequency'] = 1\n",
    "    \n",
    "    modelPars['LR_pre'] = 1e-1\n",
    "    modelPars['LR'] = 1e-2\n",
    "    modelPars['minLR'] = 1e-3\n",
    "    \n",
    "    modelPars['lrFactorGate'] = 1e0\n",
    "    modelPars['lrFactorGPRmean'] = 1e0\n",
    "    modelPars['lrFactorGPRhyperparameters'] = 2e-1\n",
    "    modelPars['lrFactorIPlocations'] = 1e-2 # I have no good experience with adapting inducing point positions at all\n",
    "    \n",
    "    modelPars['weightDecay'] = 0.\n",
    "    modelPars['jitter'] = 0.\n",
    "    modelPars['cholJitter'] = 1e-16\n",
    "    modelPars['doublePrecision'] = False\n",
    "    \n",
    "    modelPars['kernel'] = 'gaussian'\n",
    "    modelPars['applyARD'] = False\n",
    "    modelPars['has_global_noise'] = True\n",
    "    modelPars['has_task_noise'] = False\n",
    "    \n",
    "    # global model hyperpars\n",
    "    modelPars['globalModel'] = 'exactGP'\n",
    "    modelPars['preTrainEpochsGlobal'] = 1\n",
    "    modelPars['smallARDbandwidthPenalty'] = 0.\n",
    "    \n",
    "    modelPars['numIPsGlobal'] = None\n",
    "    modelPars['batchSizeGlobal'] = None\n",
    "    \n",
    "    modelPars['initMeanGlobal'] = None\n",
    "    modelPars['initSigmaGlobal'] = 1e0\n",
    "    modelPars['initLambdaGlobal'] = 1e0\n",
    "    modelPars['initNoiseLevelGlobal'] = 1e0\n",
    "    modelPars['initARDscales'] = None\n",
    "    \n",
    "    modelPars['fixedNoiseGlobal'] = False\n",
    "    modelPars['fixedMeanGlobal'] = False\n",
    "    modelPars['fixedLambdaGlobal'] = False\n",
    "    modelPars['fixedSigmaGlobal'] = False\n",
    "    modelPars['fixedIPlocationsGlobal'] = True\n",
    "    modelPars['inducingCovarTypeGlobal'] = 'scalar' # 'scalar', 'diag' or 'full'\n",
    "    \n",
    "    # expert model hyperpars\n",
    "    modelPars['expertModel'] = 'exactGP'\n",
    "    modelPars['preTrainEpochsExpert'] = int(2**6)\n",
    "    \n",
    "    modelPars['numIPsExpert'] = None\n",
    "    modelPars['batchSizeExpert'] = None\n",
    "    \n",
    "    modelPars['initMeanExpert'] = None\n",
    "    modelPars['initLambdaExpert'] = 1e0\n",
    "    modelPars['initNoiseLevelExpert'] = 1e0\n",
    "    \n",
    "    modelPars['fixedNoiseExpert'] = False\n",
    "    modelPars['fixedMeanExpert'] = False\n",
    "    modelPars['fixedLambdaExpert'] = True\n",
    "    modelPars['fixedIPlocationsExpert'] = True\n",
    "    modelPars['inducingCovarTypeExpert'] = 'scalar' # 'scalar', 'diag' or 'full'\n",
    "    \n",
    "    # gate model hyperpars\n",
    "    modelPars['numIPsGate'] = None\n",
    "    modelPars['batchSizeGate'] = None\n",
    "    \n",
    "    modelPars['initSigmaGate'] = 1e0\n",
    "    modelPars['initLambdaGate'] = 1e0\n",
    "    \n",
    "    modelPars['fixedSigmaGate'] = True\n",
    "    modelPars['fixedLambdaGate'] = True\n",
    "    modelPars['fixedIPlocationsGate'] = True\n",
    "    modelPars['gateOutputType'] = 'independent' #'independent', 'dependent'\n",
    "    modelPars['inducingCovarTypeGate'] = 'scalar' # 'scalar', 'diag' or 'full'\n",
    "    \n",
    "    # MoE model hyperpars\n",
    "    modelPars['fixedExperts'] = False\n",
    "    modelPars['expertBandwidths'] = []\n",
    "    modelPars['smallBandwidthPenalty'] = 0.\n",
    "    modelPars['expertHardSparsity'] = len(modelPars['expertBandwidths']) # the final maximal number of experts the gate chooses; note that we loose continuity of the regressor, iff chosen too small\n",
    "    modelPars['sparsifyAfterValiationIterations'] = 2\n",
    "    modelPars['initSparsificationAfterValiationIterations'] = 5\n",
    "    modelPars['noisy_gating'] = True\n",
    "    modelPars['noisy_gating_decaying'] = 0.5 #0.\n",
    "    modelPars['noisy_gating_b'] = False\n",
    "    modelPars['noisy_gating_w'] = False\n",
    "    modelPars['gate_noise_stdDev'] = 1e-1\n",
    "    \n",
    "    return modelPars\n",
    "\n",
    "modelPars = initModelPars()\n",
    "\n",
    "# general model design and training\n",
    "modelPars['maxEPOCH'] = 1000\n",
    "modelPars['subValidationFrequency'] = 2\n",
    "modelPars['lrPre'] = 1e-1\n",
    "modelPars['lr'] = 1e-2\n",
    "modelPars['min_lr'] = 1e-3\n",
    "\n",
    "modelPars['lrFactorGate'] = 2e0\n",
    "modelPars['lrFactorGPRmean'] = 1e0\n",
    "modelPars['lrFactorGPRhyperparameters'] = 2e-1\n",
    "modelPars['lrFactorIPlocations'] = 0. # I have no good experience with adapting inducing point positions at all\n",
    "\n",
    "modelPars['jitter'] = 1e-11 #0.\n",
    "modelPars['cholJitter'] = 1e-11\n",
    "modelPars['doublePrecision'] = True\n",
    "\n",
    "# global model hyperpars\n",
    "modelPars['initSigmaGlobal'] = 1e-2\n",
    "modelPars['numIPsGlobal'] = int(2**9)\n",
    "modelPars['batchSizeGlobal'] = int(2**8)\n",
    "\n",
    "# expert model hyperpars\n",
    "modelPars['numIPsExpert'] = int(2**9)\n",
    "modelPars['batchSizeExpert'] = int(2**8)\n",
    "\n",
    "# gate model hyperpars\n",
    "modelPars['numIPsGate'] = int(2**7)\n",
    "modelPars['batchSizeGate'] = int(2**9)\n",
    "modelPars['initSigmaGate'] = 5e-2\n",
    "modelPars['initLambdaGate'] = 1e1\n",
    "\n",
    "# MoE model hyperpars\n",
    "modelPars['smallBandwidthPenalty'] = 5e-1\n",
    "modelPars['expertBandwidths'] = 10**np.linspace(-9/3,-3/3,7)\n",
    "modelPars['expertHardSparsity'] = 2\n",
    "\n",
    "def hyperparameterHeuristics(n, expPars, alPars, ipSelectionPars, modelPars):\n",
    "    refTrainSize = expPars['trainingSizes'][0]\n",
    "    trainSizeAdaptFactor = n/refTrainSize\n",
    "    # init/update some hyperparameters according to an educated heuristic depending on n!\n",
    "    pretrainEpochsExpert_base = int(2**6)\n",
    "    pretrainEpochsExpert_ScalingExponent = -5/6\n",
    "    modelPars['pretrainEpochsExpert'] = max(1, int(np.round(pretrainEpochsExpert_base * trainSizeAdaptFactor**pretrainEpochsExpert_ScalingExponent)))\n",
    "    \n",
    "    initLambdaExpert_base = 1e1\n",
    "    expertLambda_ScalingExponent = -0.5\n",
    "    modelPars['initLambdaExpert'] = initLambdaExpert_base * trainSizeAdaptFactor**expertLambda_ScalingExponent\n",
    "    modelPars['initLambdaGlobal'] = initLambdaExpert_base * trainSizeAdaptFactor**expertLambda_ScalingExponent\n",
    "    \n",
    "    validationFrequency_ScalingExponent = -0.5\n",
    "    validationFrequency_base = 16\n",
    "    modelPars['validationFrequency'] = max(1, int(np.round(validationFrequency_base * trainSizeAdaptFactor**validationFrequency_ScalingExponent)))\n",
    "    \n",
    "    return alPars, ipSelectionPars, modelPars\n",
    "\n",
    "resultFile = os.path.join(saveDir, 'testResultFile.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDoppler(saberModel, plotGate = True, plotIsolatedExperts = True):\n",
    "        \n",
    "    gateTrainState = saberModel.model.gate.training\n",
    "    expertTrainState = saberModel.model.experts.training\n",
    "    expertLikelihoodState = saberModel.model.likelihood.training\n",
    "    \n",
    "    saberModel.model.gate.eval()\n",
    "    if not saberModel.pars['fixedExperts']:\n",
    "        #if self.pars['expertModel'] == 'variationalGP' or looScenario:\n",
    "        saberModel.model.experts.eval()\n",
    "        saberModel.model.likelihood.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction, expert_output_distributions, gateValues, inxes, sigmaPlot = saberModel(torch.from_numpy(xPlot).type(saberModel.yExpert.dtype), returnSigmaPredictions = True, combine = False, predictionsOnly = True)\n",
    "    \n",
    "    if gateTrainState:\n",
    "        saberModel.model.gate.train()\n",
    "    if expertTrainState:\n",
    "        saberModel.model.experts.train()\n",
    "    if expertLikelihoodState:\n",
    "        saberModel.model.likelihood.train()\n",
    "\n",
    "    plotResiduals = False\n",
    "    if plotResiduals:\n",
    "        logResidualsPlot = np.log(np.abs(prediction.data.numpy() - yPlot))\n",
    "    fig, ax = plt.subplots(2 + plotGate + plotIsolatedExperts, 1)\n",
    "    fig.set_figheight(8*(2 + plotGate + plotIsolatedExperts))\n",
    "    fig.set_figwidth(16)\n",
    "    ax[0].set_xlabel('x', fontsize=24)\n",
    "    ax[0].set_xlim(0,1)\n",
    "    ax[0].set_ylim(-14, 14)\n",
    "    ax[0].plot(xPlot, yPlot, 'k-',label='f')\n",
    "    ax[0].plot(xPlot, prediction.data.numpy(), 'r--', lw=3,label='$\\widehat{f}$')\n",
    "    ax[0].scatter(saberModel.xGate, saberModel.yGate, color = \"blue\", alpha=0.1,label='y')\n",
    "    ax[0].plot(xPlot, np.log(sigmaPlot.data.numpy()), 'g-', lw=3,label='$\\widehat{\\sigma}$')\n",
    "    if plotResiduals:\n",
    "        ax[0].plot(xPlot, logResidualsPlot, 'r-', lw=3,label='$|\\widehat{f}-y|$')\n",
    "    \n",
    "    cmap = cm.get_cmap('jet',len(saberModel.model.experts))\n",
    "    \n",
    "    ############\n",
    "    ax[1].set_xlabel('log-scale x', fontsize=24)\n",
    "    ax[1].set_xlim(0.001, 1.)\n",
    "    ax[1].set_ylim(-14, 14)\n",
    "    ax[1].semilogx(xPlot, yPlot, 'k-',label='f')\n",
    "    ax[1].plot(xPlot, prediction.data.numpy(), 'r--', lw=3,label='$\\widehat{f}$')\n",
    "    ax[1].scatter(saberModel.xGate, saberModel.yGate, color = \"blue\", alpha=0.1,label='y')\n",
    "    ax[1].plot(xPlot, np.log(sigmaPlot.data.numpy()), 'g-', lw=3,label='$\\widehat{\\sigma}$')\n",
    "    if plotResiduals:\n",
    "        ax[1].plot(xPlot, logResidualsPlot, 'r-', lw=3,label='$|\\widehat{f}-y|$')\n",
    "    ############\n",
    "    ax[1].legend(loc='upper left', fontsize = 20)\n",
    "    \n",
    "    if saberModel.pars['expertModel'] == 'variationalGP':\n",
    "        expertIP = saberModel.model.experts[j].model.variational_strategy.inducing_points.detach().numpy().ravel()\n",
    "        ax[1].scatter(expertIP, expertIP*0, color = cmap(0), label='expert IP', s = 25, marker='x')\n",
    "        for j in range(1, len(saberModel.model.experts)):\n",
    "            expertIP = saberModel.model.experts[j].model.variational_strategy.inducing_points.detach().numpy().ravel()\n",
    "            ax[1].scatter(expertIP, expertIP*0 + j, color = cmap(j), label='expert IP', s = 25, marker='x')\n",
    "            \n",
    "    if saberModel.pars['expertModel'] == 'exactGP' and hasattr(saberModel.model.experts[0].model.covar_module, 'inducing_points'):\n",
    "        expertIP = saberModel.model.experts[0].model.covar_module.inducing_points.detach().numpy().ravel()\n",
    "        ax[1].scatter(expertIP, expertIP*0, color = cmap(0), label='expert IP', s = 25, marker='x')\n",
    "        for j in range(1,len(saberModel.model.experts)):\n",
    "            expertIP = saberModel.model.experts[j].model.covar_module.inducing_points.detach().numpy().ravel()\n",
    "            ax[1].scatter(expertIP, expertIP*0 + j, color = cmap(j), s = 25, marker='x')\n",
    "        \n",
    "    gateIP = saberModel.model.gate.variational_strategy.base_variational_strategy.inducing_points.detach().numpy()[0].ravel()\n",
    "    ax[1].scatter(gateIP, gateIP*0-5, color = 'm', label='gate IP', s = 25, marker='x')\n",
    "    \n",
    "    i = 1\n",
    "    if plotGate:\n",
    "        i += 1\n",
    "        ax[i].set_xlabel('log-scale x', fontsize=24)\n",
    "        ax[i].set_xlim(0.001, 1.)\n",
    "        ax[i].set_ylim(-0.1, 1.1)\n",
    "        for j in range(len(saberModel.model.experts)):\n",
    "            if len(inxes[j]) == 0:\n",
    "                continue\n",
    "            \n",
    "            gateVals = np.nan*np.zeros([len(xPlot)])\n",
    "            gateVals[inxes[j]] = gateValues[j].squeeze()\n",
    "            ax[i].semilogx(xPlot, gateVals, color=cmap(j))\n",
    "    if plotIsolatedExperts:\n",
    "        i += 1\n",
    "        ax[i].set_xlabel('log-scale x', fontsize=24)\n",
    "        ax[i].set_xlim(0.001, 1.)\n",
    "        ax[i].set_ylim(-14, 14)\n",
    "        for j in range(len(saberModel.model.experts)):\n",
    "            if len(inxes[j]) == 0:\n",
    "                continue\n",
    "\n",
    "            expertPred = np.nan*np.zeros([len(xPlot)])\n",
    "            expertPred[inxes[j]] = expert_output_distributions[j].mean\n",
    "            ax[i].semilogx(xPlot, expertPred, '-', color=cmap(j))\n",
    "            \n",
    "    plt.draw()\n",
    "    plt.pause(2)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPars['globalModel'] = 'exactGP'\n",
    "modelPars['fixedNoiseGlobal'] = True\n",
    "modelPars['fixedMeanGlobal'] = True\n",
    "modelPars['fixedLambdaGlobal'] = True\n",
    "modelPars['fixedSigmaGlobal'] = True\n",
    "\n",
    "modelPars['expertModel'] = 'exactGP'\n",
    "modelPars['fixedExperts'] = True\n",
    "modelPars['fixedMeanExpert'] = True\n",
    "modelPars['fixedLambdaExpert'] = True\n",
    "\n",
    "modelPars['lr'] = 1e-2\n",
    "modelPars['min_lr'] = 1e-3\n",
    "\n",
    "modelPars['lrFactorGate'] = 2e0\n",
    "modelPars['lrFactorGPRmean'] = 1e0\n",
    "modelPars['lrFactorGPRhyperparameters'] = 1e0#2e-1\n",
    "\n",
    "\n",
    "modelPars['numIPsGlobal'] = int(2**9)\n",
    "modelPars['numIPsExpert'] = int(2**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# outside loop: specify for each hyperparameter some initial value and a flag whether to tune the parameter or not\n",
    "pInit = inputDensityUnif\n",
    "pInitPlot = pInit(xPlot, alPars['inputSpaceBounds'])\n",
    "if alPars['initialSamplingStrategy'] == 'random':\n",
    "    randPInit = randInputUnif\n",
    "elif alPars['initialSamplingStrategy'] == 'equi':\n",
    "    randPInit = equidistantInput\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "labelOracle = noisyLabelFun\n",
    "randUniform = randInputUnif\n",
    "xRef = xPlot\n",
    "\n",
    "resSABERopt_SABER = resultList(resultFile=resultFile)\n",
    "\n",
    "for rep in range(expPars['repetitions']):\n",
    "    \n",
    "    clear_output()\n",
    "    ####### set up global stuff for this repetition here (like validation and test data) ######\n",
    "    seed = rep*(4*expPars['iters']+1)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed+1)\n",
    "    validationX = randInputUnif(expPars['validationSize'], inputSpaceBounds)\n",
    "    validationY = noisyLabelFun(validationX)\n",
    "    iwVal = None\n",
    "    valInx = None\n",
    "    valLoss = 'mll'\n",
    "        \n",
    "    xTest = xPlot\n",
    "    fTest = labelFun(xTest)\n",
    "    testInx = None\n",
    "    \n",
    "    resSABERopt_SABER.update(rep, valX = validationX, valY = validationY, testX = xTest, testY = fTest, valInx = None, testInx = None)\n",
    "    resSABERopt_SABER.save()\n",
    "    \n",
    "    # set up the learner\n",
    "    saberAL = saberActiveLearner(expPars, alPars, testDensity = testDensity, randTestDistribution = randTestDistribution, randPInit = randPInit, pInit = pInit, randUniform = randUniform, labelOracle = labelOracle, xRef = xRef)\n",
    "    \n",
    "    if rep < len(resSABERopt_SABER['allXs']):\n",
    "        # check if this repetition is fully calculated\n",
    "        if len(resSABERopt_SABER['allXs'][rep]) >= expPars['trainingSizes'][-1] and len(resSABERopt_SABER['allSABERmodelsStateDict']) > rep and len(resSABERopt_SABER['allSABERmodelsStateDict'][rep]) >= expPars['iters']:\n",
    "            continue\n",
    "            \n",
    "        # load so far calculated data\n",
    "        saberAL.importPreviousData(resSABERopt_SABER['allXs'][rep], resSABERopt_SABER['allYs'][rep], densitiesInformation = resSABERopt_SABER['allDensitiesInformation'][rep])\n",
    "        \n",
    "    for j in range(expPars['iters']):\n",
    "        n = expPars['trainingSizes'][j]\n",
    "        if n < len(saberAL.X):\n",
    "            # case: we already have data and model for this iteration\n",
    "            continue\n",
    "        \n",
    "        if n == len(saberAL.X) and len(resSABERopt_SABER['allSABERmodelsStateDict']) > rep and len(resSABERopt_SABER['allSABERmodelsStateDict'][rep]) > j:\n",
    "            # case: we already have data and model for this iteration\n",
    "            continue\n",
    "########## Summarize in Pserudo-Code the a whole step of the AL Framework ##############\n",
    "###################### prior training updates ####################\n",
    "        \n",
    "        ##### sample a new batch according to current optimal density proposal ###########\n",
    "        seed = rep*(4*expPars['iters']+1) + j + 1\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed+1)\n",
    "        if n > len(saberAL.X):\n",
    "            \n",
    "            # set up current training distribution\n",
    "            saberAL.updateTrainingDistribution()\n",
    "            \n",
    "            sampleSize = n - len(saberAL.X)\n",
    "            \n",
    "            saberAL.growTrainingData(sampleSize)\n",
    "            \n",
    "            resSABERopt_SABER.update(rep, X = saberAL.X, y = saberAL.y, densitiesInformation = saberAL.densitiesInformation, trainInx = saberAL.trainInx)\n",
    "            resSABERopt_SABER.save()\n",
    "            \n",
    "            saberAL.updateTrainImportanceWeights()\n",
    "            \n",
    "        #estimate pTrainPlot:\n",
    "        pTrainPlot = pInitPlot\n",
    "        if saberAL.alPars['updateTrainingDensity'] and len(saberAL.densitiesInformation) > 1:\n",
    "            for i in range(len(saberAL.densitiesInformation) - 1):\n",
    "                w = saberAL.densitiesInformation[i]['densityWeight']\n",
    "                pTrainPlot = pTrainPlot * w + resSABERopt_SABER['allPlotInformation'][rep][i]['proposalDensityPlot'] * (1-w)\n",
    "            \n",
    "        # apply hyperparameter heuristics to update some parameters based on train size\n",
    "        alPars, ipSelectionPars, modelPars = hyperparameterHeuristics(n, expPars, alPars, ipSelectionPars, modelPars)\n",
    "            \n",
    "        # get the new IP selection strategies\n",
    "        #inducing_point_method_expert, inducing_point_method_gate = defineIPsamplingMethod(saberAL, ipSelectionPars, alPars, modelPars, expertTrainingSubInx, gateTrainingSubInx)\n",
    "        \n",
    "        ipMetaSamplerPars = initIPsamplerPars(ipSelectionPars, alPars, modelPars)\n",
    "        ipMetaSampler = inducingPointSampler(saberAL, pars = ipMetaSamplerPars)\n",
    "        \n",
    "        # if needed, estimate a global model to estimate shared expert parameters\n",
    "        seed = rep*(4*expPars['iters']+1) + expPars['iters'] + j + 1\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed+1)\n",
    "        \n",
    "        if not alPars['noiseFree'] and not alPars['homoscedastic'] and localNoiseVariance is None:\n",
    "            raise NotImplementedError\n",
    "            # TODO: Need to provide a data-driven localNoiseVariance function estimate\n",
    "            \n",
    "        # define a global model and tune unspecified hyperparameters on validation set\n",
    "        globalGPRpars = initGPRpars(modelPars, alPars)\n",
    "        \n",
    "        globalModel = globalGPR(saberAL.X, saberAL.y, iwTrain = saberAL.importanceWeights, lossFunction = None, pars = globalGPRpars, xVal = validationX, yVal = validationY, iwVal = iwVal, valLoss = valLoss, localNoiseVariance = localNoiseVariance, labelModel = expPars['labelModel'], derivativeModel = expPars['derivativeModel'])\n",
    "        \n",
    "        # check if we already got a tuned global model\n",
    "        if len(resSABERopt_SABER['allGlobalModelsStateDict']) > rep and len(resSABERopt_SABER['allGlobalModelsStateDict'][rep]) > j:\n",
    "            globalStateDict = resSABERopt_SABER['allGlobalModelsStateDict'][rep][j]\n",
    "        else:\n",
    "            globalStateDict = None\n",
    "        inducing_point_method_global = ipMetaSampler(IPstrategy = ipSelectionPars['IPstrategyExpert'])\n",
    "        \n",
    "        globalModel.init(inducing_points = None, inducing_point_method = inducing_point_method_global, modelStateDict = globalStateDict)\n",
    "        if globalStateDict is None:\n",
    "            globalModel.preTrainModel()\n",
    "            globalModel.trainModel()\n",
    "\n",
    "            resSABERopt_SABER.update(rep, globalModel_stateDict = globalModel.state_dict())\n",
    "            resSABERopt_SABER.save()\n",
    "        \n",
    "        if len(resSABERopt_SABER['allGlobalTestErrors']) <= rep or len(resSABERopt_SABER['allGlobalTestErrors'][rep]) <= j:\n",
    "            with torch.no_grad():\n",
    "                testPredGlobal = globalModel(torch.from_numpy(xTest).type(globalModel.y.dtype), predictionsOnly = True).mean\n",
    "            testPredGlobal = testPredGlobal.numpy()\n",
    "            testMAEglobal = np.mean(np.abs(testPredGlobal - fTest))\n",
    "            testRMSEglobal = np.mean((testPredGlobal - fTest)**2)**0.5\n",
    "            testMaxAEglobal = np.max(np.abs(testPredGlobal - fTest))\n",
    "\n",
    "            globalTestErrors = {'RMSE': testRMSEglobal, 'MAE': testMAEglobal, 'maxAE': testMaxAEglobal}\n",
    "\n",
    "            resSABERopt_SABER.update(rep, globalTestErrors = globalTestErrors)\n",
    "            resSABERopt_SABER.save()\n",
    "        \n",
    "        print('median global RMSE')\n",
    "        print(np.nanmedian(numpy_fillna([[err['RMSE'] for err in errors] for errors in resSABERopt_SABER['allGlobalTestErrors']]),0))\n",
    "        print('median global MAE')\n",
    "        print(np.nanmedian(numpy_fillna([[err['MAE'] for err in errors] for errors in resSABERopt_SABER['allGlobalTestErrors']]),0))\n",
    "        print('median global max AE')\n",
    "        print(np.nanmedian(numpy_fillna([[err['maxAE'] for err in errors] for errors in resSABERopt_SABER['allGlobalTestErrors']]),0))\n",
    "        \n",
    "        #TODO: transfer some globally estimated parameters to init SABER model\n",
    "        \n",
    "        SABERpars = initSABERpars(expPars, modelPars, alPars)\n",
    "        saberModel = SABER(saberAL.X, saberAL.y, iwTrain = saberAL.importanceWeights, lossFunction = None, pars = SABERpars, xVal = validationX, yVal = validationY, iwVal = iwVal, valLoss = valLoss, localNoiseVariance = localNoiseVariance)\n",
    "        \n",
    "        saberModel.init(ipMetaSampler, ipSelectionPars, modelStateDict = None)\n",
    "        saberModel.trainModel(printMethod = printDoppler, plotFrequencyFactor = modelPars['plotFrequencyFactor'])\n",
    "        \n",
    "        saberAL.updateLocalProperties(saberModel.localBandwidthFunction, saberModel.localNoiseVarianceEstimate)\n",
    "        \n",
    "        plotInf = {}\n",
    "        plotInf['trainingDensityPlot'] = pTrainPlot\n",
    "        with torch.no_grad(), gpytorch.settings.fast_computations(log_prob=False, covar_root_decomposition=False):\n",
    "            plotInf['sigmaPlot'] = saberAL.localBandwidthEstimate(xPlot)\n",
    "            plotInf['complexityPlot'] = saberAL.localFunctionComplexityEstimate(xPlot, pCurrentOfX = pTrainPlot, bandwidthOfX = plotInf['sigmaPlot'])\n",
    "            plotInf['proposalDensityPlot'] = saberAL.proposalDensityEstimate(xPlot, pCurrentOfX = pTrainPlot, q = testDensityPlot, lfc = plotInf['complexityPlot'])\n",
    "        \n",
    "        resSABERopt_SABER.update(rep, densitiesInformation = saberAL.densitiesInformation, SABERmodel_stateDict = saberModel.state_dict(), plotInformation = plotInf)\n",
    "        resSABERopt_SABER.save()\n",
    "        \n",
    "        # measure performance\n",
    "        with torch.no_grad():\n",
    "            testPred, = saberModel(torch.from_numpy(xTest).type(saberModel.model.gate.variational_strategy.base_variational_strategy.inducing_points.dtype), predictionsOnly = True)\n",
    "        testPred = testPred.numpy()\n",
    "        \n",
    "        testMAE = np.mean(np.abs(testPred - fTest))\n",
    "        testRMSE = np.mean((testPred - fTest)**2)**0.5\n",
    "        testMaxAE = np.max(np.abs(testPred - fTest))\n",
    "        testErrors = {'RMSE': testRMSE, 'MAE': testMAE, 'maxAE': testMaxAE}\n",
    "        \n",
    "        resSABERopt_SABER.update(rep, testErrors = testErrors)\n",
    "        resSABERopt_SABER.save()\n",
    "        \n",
    "        print('median RMSE')\n",
    "        print(np.nanmedian(numpy_fillna([[err['RMSE'] for err in errors] for errors in resSABERopt_SABER['allTestErrors']]),0))\n",
    "        print('median MAE')\n",
    "        print(np.nanmedian(numpy_fillna([[err['MAE'] for err in errors] for errors in resSABERopt_SABER['allTestErrors']]),0))\n",
    "        print('median max AE')\n",
    "        print(np.nanmedian(numpy_fillna([[err['maxAE'] for err in errors] for errors in resSABERopt_SABER['allTestErrors']]),0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xTest, testPredGlobal)\n",
    "plt.plot(xTest, fTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalGPRpars['numIPs'] = 256\n",
    "globalGPRpars['model'] = 'variationalGP'\n",
    "globalGPRpars['fixedMean'] = False\n",
    "globalGPRpars['fixedNoise'] = False\n",
    "globalGPRpars['fixedLambda'] = False\n",
    "globalGPRpars['fixedSigma'] = False\n",
    "globalGPRpars['fixedIPlocations'] = True\n",
    "globalGPRpars['preTrainEpochs'] = 16\n",
    "globalGPRpars['lrFactorGPRhyperparameters'] = 0.2\n",
    "globalModel = globalGPR(saberAL.X, saberAL.y, iwTrain = saberAL.importanceWeights, lossFunction = None, pars = globalGPRpars, xVal = validationX, yVal = validationY, iwVal = iwVal, valLoss = valLoss, localNoiseVariance = localNoiseVariance, labelModel = expPars['labelModel'], derivativeModel = expPars['derivativeModel'])\n",
    "globalModel.init(inducing_points = None, inducing_point_method = inducing_point_method_expert, modelStateDict = None)\n",
    "globalModel.preTrain()\n",
    "\n",
    "with torch.no_grad():\n",
    "    testPredGlobal = globalModel(torch.from_numpy(xTest).type(globalModel.y.dtype), predictionsOnly = True).mean\n",
    "testPredGlobal = testPredGlobal.numpy()\n",
    "testMAEglobal = np.mean(np.abs(testPredGlobal - fTest))\n",
    "testRMSEglobal = np.mean((testPredGlobal - fTest)**2)**0.5\n",
    "testMaxAEglobal = np.max(np.abs(testPredGlobal - fTest))\n",
    "\n",
    "# show some statistics\n",
    "#clear_output()\n",
    "\n",
    "print('median RMSE')\n",
    "print(testRMSEglobal)\n",
    "print('median MAE')\n",
    "print(testMAEglobal)\n",
    "print('median max AE')\n",
    "print(testMaxAEglobal)\n",
    "        \n",
    "plt.plot(xTest, testPredGlobal)\n",
    "plt.plot(xTest, fTest)\n",
    "plt.pause(1)\n",
    "\n",
    "globalModel.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    testPredGlobal = globalModel(torch.from_numpy(xTest).type(globalModel.y.dtype), predictionsOnly = True).mean\n",
    "testPredGlobal = testPredGlobal.numpy()\n",
    "testMAEglobal = np.mean(np.abs(testPredGlobal - fTest))\n",
    "testRMSEglobal = np.mean((testPredGlobal - fTest)**2)**0.5\n",
    "testMaxAEglobal = np.max(np.abs(testPredGlobal - fTest))\n",
    "\n",
    "# show some statistics\n",
    "#clear_output()\n",
    "\n",
    "print('median RMSE')\n",
    "print(testRMSEglobal)\n",
    "print('median MAE')\n",
    "print(testMAEglobal)\n",
    "print('median max AE')\n",
    "print(testMaxAEglobal)\n",
    "        \n",
    "plt.plot(xTest, testPredGlobal)\n",
    "plt.plot(xTest, fTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SABER optimal sampling\n",
    "\n",
    "resSABERopt_SABER = initResultListSABER(repetitions, iters, resultFile)\n",
    "\n",
    "for rep in range(repetitions):\n",
    "    \n",
    "    clear_output()\n",
    "    ####### set up global stuff for this repetition here (like validation and test data) ######\n",
    "    seed = rep*(4*iters+1)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed+1)\n",
    "    validationX = randInputUnif(validationSize, inputSpaceBounds)\n",
    "    validationY = noisyLabelFun(validationX)\n",
    "        \n",
    "    xTest = xPlot\n",
    "    fTest = labelFun(xTest)\n",
    "    \n",
    "    # set up the learner\n",
    "    saberAL = saberActiveLearner(nnPars, inputSpaceBounds, noisyLabelFun, testDensity, randTestDistribution, refTrainSize, 0, xRef = xRef, Q = Q, randPInit = randPInit, pInit = inputDensityUnif, xVal = validationX, yVal = validationY, updateTrainingDensity = updateTrainingDensity)\n",
    "    \n",
    "    # load so far estimated global models of this repetition\n",
    "    if len(allGlobalModels) > rep:\n",
    "        globalModelsOfRep = allGlobalModels[rep]\n",
    "    else:\n",
    "        globalModelsOfRep = []\n",
    "        allGlobalModels.append(globalModelsOfRep)\n",
    "        \n",
    "    if len(allGlobalLosses) > rep:\n",
    "        globalLossesOfRep = allGlobalLosses[rep]\n",
    "        globalValLossesOfRep = allGlobalValLosses[rep]\n",
    "        globalErrorsOfRep = allGlobalErrors[rep]\n",
    "    else:\n",
    "        globalLossesOfRep = []\n",
    "        globalValLossesOfRep = []\n",
    "        globalErrorsOfRep = []\n",
    "        allGlobalLosses.append(globalLossesOfRep)\n",
    "        allGlobalValLosses.append(globalValLossesOfRep)\n",
    "        allGlobalErrors.append(globalErrorsOfRep)\n",
    "        \n",
    "    # load so far pre-trained expert models of this repetition\n",
    "    if rep < len(allInitialExperts):\n",
    "        initialExpertsOfRep = allInitialExperts[rep]\n",
    "    else:\n",
    "        initialExpertsOfRep = []\n",
    "        allInitialExperts.append(initialExpertsOfRep)\n",
    "    \n",
    "    if rep < len(resSABERopt_SABER['allXs']):\n",
    "        # check if this repetition is fully calculated\n",
    "        if len(resSABERopt_SABER['allXs'][rep]) >= trainingSizes[-1]  and len(resSABERopt_SABER['allSABERmodelsStateDict']) > rep and len(resSABERopt_SABER['allSABERmodelsStateDict'][rep]) >= iters:\n",
    "            continue\n",
    "            \n",
    "        # load so far calculated data\n",
    "        saberAL.importPreviousData(resSABERopt_SABER['allXs'][rep], resSABERopt_SABER['allYs'][rep], densitiesInformation = resSABERopt_SABER['allDensitiesInformation'][rep], SABERmodelsStateDict = resSABERopt_SABER['allSABERmodelsStateDict'][rep] if len(resSABERopt_SABER['allSABERmodelsStateDict']) > rep else [])\n",
    "            \n",
    "\n",
    "    for j in range(iters):\n",
    "        n = trainingSizes[j]\n",
    "        if n < len(saberAL.X):\n",
    "            # case: we already have data and model for this iteration\n",
    "            continue\n",
    "        \n",
    "        if n == len(saberAL.X) and len(resSABERopt_SABER['allSABERmodelsStateDict']) > rep and len(resSABERopt_SABER['allSABERmodelsStateDict'][rep]) > j:\n",
    "            # case: we already have data and model for this iteration\n",
    "            continue\n",
    "        \n",
    "        ##### sample a new batch according to current optimal density proposal ###########\n",
    "        seed = rep*(4*iters+1) + j + 1\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed+1)\n",
    "        if n > len(saberAL.X):\n",
    "            \n",
    "            # set up current training distribution\n",
    "            saberAL.updateTrainingDistribution()\n",
    "            \n",
    "            batchSize = n - len(saberAL.X)\n",
    "            \n",
    "            saberAL.growTrainingData(batchSize)\n",
    "            updateTrainSABER(resSABERopt_SABER, rep, saberAL.X, saberAL.y, saberAL.densitiesInformation)\n",
    "            saveALresultSABER(resultFile, resSABERopt_SABER)\n",
    "        \n",
    "        # if needed, estimate a global model to estimate shared expert parameters\n",
    "        seed = rep*(4*iters+1) + iters + j + 1\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed+1)\n",
    "        if len(globalModelsOfRep) > j:\n",
    "            globalModel = globalModelsOfRep[j]\n",
    "            globalErrors = globalErrorsOfRep[j]\n",
    "        else:\n",
    "            globalModel, global_loss_str, global_val_loss_str = saberAL.trainSABERModel(printMethod = printMethodSABER, onlyFindGlobalModel = True)\n",
    "            \n",
    "            globalModelsOfRep.append(globalModel)\n",
    "            globalLossesOfRep.append(global_loss_str)\n",
    "            globalValLossesOfRep.append(global_val_loss_str)\n",
    "            \n",
    "            allGlobalModels[rep] = globalModelsOfRep\n",
    "            allGlobalLosses[rep] = globalLossesOfRep\n",
    "            allGlobalValLosses[rep] = globalValLossesOfRep\n",
    "            \n",
    "            if not globalModel is None:\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    testPred = saberAL.globalModel(torch.from_numpy(xTest).type(saberAL.globalModel.variational_strategy.inducing_points.dtype), predictionsOnly = True, labelsOnly = False, jitter = saberAL.nnPars['jitter'], cholJitter = saberAL.nnPars['cholJitter']).mean\n",
    "\n",
    "                if testPred.ndim < 2:\n",
    "                    testPred = testPred.unsqueeze(-1)\n",
    "                testPred = testPred.numpy()\n",
    "\n",
    "                testRMSEglobal = np.mean((testPred - fTest)**2)**0.5\n",
    "                testMAEglobal = np.mean(np.abs(testPred - fTest))\n",
    "                testmaxAEGlobal = np.max(np.abs(testPred - fTest))\n",
    "\n",
    "            else:\n",
    "                testRMSEglobal = None\n",
    "                testMAEglobal = None\n",
    "                testmaxAEGlobal = None\n",
    "            globalErrors = {'testRMSE': testRMSEglobal, 'testMAE': testMAEglobal, 'testMaxAE': testmaxAEGlobal}\n",
    "            globalErrorsOfRep.append(globalErrors)\n",
    "            allGlobalErrors[rep] = globalErrorsOfRep\n",
    "\n",
    "            torch.save({'allGlobalModels': allGlobalModels, 'allGlobalLosses': allGlobalLosses, 'allGlobalValLosses': allGlobalValLosses, 'allGlobalErrors': allGlobalErrors}, initGlobalfile)\n",
    "\n",
    "        if not globalModel is None:\n",
    "            print('global test RMSE', globalErrors['testRMSE'])\n",
    "            print('global test MAE', globalErrors['testMAE'])\n",
    "            print('global test maxAE', globalErrors['testMaxAE'])\n",
    "            \n",
    "        if not saberAL.updateTrainingDensity and onlyCalculateGlobal:\n",
    "            continue\n",
    "            \n",
    "        if saberAL.nnPars['expertModel'] == 'sparseGP':\n",
    "            # initialize experts:\n",
    "            seed = rep*(4*iters+1) + 2*iters + j + 1\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed+1)\n",
    "            if len(initialExpertsOfRep) > j:\n",
    "                preTrainedExperts = initialExpertsOfRep[j]\n",
    "            else:\n",
    "                preTrainedExperts = saberAL.trainSABERModel(printMethod = printMethodSABER, onlyInitExperts = True, initialGlobalModel = globalModel)\n",
    "                initialExpertsOfRep.append(preTrainedExperts)\n",
    "                allInitialExperts[rep] = initialExpertsOfRep\n",
    "                torch.save({'allInitialExperts': allInitialExperts}, initExpertsfile)\n",
    "        \n",
    "        ######## estimate locally optimal bandwidths and new proposal density on reference data ####\n",
    "        seed = rep*(4*iters+1) + 3*iters + j + 1\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed+1)\n",
    "        saberAL.trainSABERModel(printMethod = printMethodSABER, initialExperts = preTrainedExperts, initialGlobalModel = globalModel)\n",
    "        ####### evalutation of local model #########################\n",
    "        \n",
    "        saberAL.updateLocalProperties()\n",
    "        \n",
    "        # measure performance\n",
    "        with torch.no_grad():\n",
    "            testPred, = saberAL.SABERmodel(torch.from_numpy(xTest).type(saberAL.SABERmodel.gate.variational_strategy.base_variational_strategy.inducing_points.dtype), predictionsOnly = True)\n",
    "        testPred = testPred.numpy()\n",
    "        testMAE = np.mean(np.abs(testPred - fTest))\n",
    "        testRMSE = np.mean((testPred - fTest)**2)**0.5\n",
    "        testMaxAE = np.max(np.abs(testPred - fTest))\n",
    "        \n",
    "        resSABERopt_SABER['allSABERrmse'][rep,j] = testRMSE\n",
    "        resSABERopt_SABER['allSABERmaxAE'][rep,j] = testMaxAE\n",
    "        \n",
    "        # show some statistics\n",
    "        #clear_output()\n",
    "        \n",
    "        print('median RMSE')\n",
    "        print(np.nanmedian(resSABERopt_SABER['allSABERrmse'],0))\n",
    "        print('median max AE')\n",
    "        print(np.nanmedian(resSABERopt_SABER['allSABERmaxAE'],0))\n",
    "        \n",
    "        # show some statistics\n",
    "        plt.clf()\n",
    "        fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "        fig.set_figheight(3.5*4)\n",
    "        fig.set_figwidth(14)\n",
    "        ax[0].plot(xPlot, saberAL.pCurrent(xPlot),'b--') # current training density\n",
    "        ax[0].plot(xPlot, saberAL.proposalDensityEstimate(xPlot),'g--') # next proposal density\n",
    "\n",
    "        ax[1].set_xlim(0.001, 1.)\n",
    "        ax[1].set_ylim(-14, 14)\n",
    "        ax[1].semilogx(xPlot, yPlot, 'k-')\n",
    "        ax[1].scatter(saberAL.X, saberAL.y, color = \"blue\", alpha=0.1)\n",
    "        ax[1].plot(xTest, testPred, 'r--', lw=3)\n",
    "        ax[1].plot(xPlot, np.log(saberAL.localBandwidthEstimate(xPlot)), \"g-\")\n",
    "\n",
    "        plt.draw()\n",
    "        plt.pause(2)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        resSABERopt_SABER = updateResultListSABER(resSABERopt_SABER, rep, saberAL.densitiesInformation, saberAL.SABERmodelsStateDict)\n",
    "        \n",
    "        saveALresultSABER(resultFile, resSABERopt_SABER)\n",
    "        \n",
    "#clear_output()\n",
    "print('median RMSE')\n",
    "print(np.nanmedian(resSABERopt_SABER['allSABERrmse'],0))\n",
    "print('median max AE')\n",
    "print(np.nanmedian(resSABERopt_SABER['allSABERmaxAE'],0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
